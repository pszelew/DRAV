{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test capacity control\n",
    "\n",
    "Test if we can infer about certain type of competences using only the desired part of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from model.encoder import CandidateEncoderConfig\n",
    "from model.decoder import CandidateDecoderConfig\n",
    "from model.candidate_vae import CandidateVAE\n",
    "from trainer.trainer import TrainerConfig\n",
    "from config.general_config import GeneralConfig\n",
    "from dataset.dataset import SellersDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = \"candidate_vae_02_06_22_04_13_30\"\n",
    "CHECKPOINT = \"7506_checkpoint.tar\"\n",
    "# If false, we can used cached content e.g. if we are testing the code\n",
    "CREATE_DATASET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(os.path.join(\"checkpoints\", EXPERIMENT, \"config.yaml\"), \"r\") as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)[\"vae\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "general_config = GeneralConfig(**config[\"general\"])\n",
    "\n",
    "\n",
    "encoder_config = CandidateEncoderConfig(**{**config[\"encoder\"], **config[\"general\"]})\n",
    "\n",
    "decoder_config = CandidateDecoderConfig(**{**config[\"decoder\"], **config[\"general\"]})\n",
    "\n",
    "trainer_config = TrainerConfig(**{**config[\"trainer\"], **config[\"general\"]})\n",
    "\n",
    "log_dir = os.path.join(general_config.checkpoints_dir, \"runs\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir $log_dir --port=6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset data/dataset_2/...\n",
      "[2022-06-02 20:53:18,885] {dataset.py:253} INFO - Loading dataset data/dataset_2/...\n",
      "Loaded dataset data/dataset_2/!\n",
      "[2022-06-02 20:53:19,465] {dataset.py:281} INFO - Loaded dataset data/dataset_2/!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataset = SellersDataset(\n",
    "    dataset_path=general_config.datset_path,\n",
    "    test_index=general_config.test_index,\n",
    "    embedder_name=general_config.embedder_name,\n",
    "    raw_data_path=general_config.raw_data_path,\n",
    "    device=DEVICE,\n",
    "    bow_remove_stopwords=general_config.bow_remove_stopwords,\n",
    "    bow_remove_sentiment=general_config.bow_remove_sentiment,\n",
    "    nn_embedding_size=encoder_config.lstm_hidden_dim,\n",
    "    trim_tr=general_config.trim_tr,\n",
    ")\n",
    "# dataset.prepare_dataset()\n",
    "dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent space configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "disentangled_targets = {\n",
    "    \"skills\": {\n",
    "        \"latent_dim\": trainer_config.skills_dim,\n",
    "        \"output_dim\": dataset.bow_vocab.n_words,\n",
    "        \"indexes\": (0, trainer_config.skills_dim),\n",
    "    },\n",
    "    \"education\": {\n",
    "        \"latent_dim\": trainer_config.education_dim,\n",
    "        \"output_dim\": dataset.bow_vocab.n_words,\n",
    "        \"indexes\": (\n",
    "            trainer_config.skills_dim,\n",
    "            trainer_config.skills_dim + trainer_config.education_dim,\n",
    "        ),\n",
    "    },\n",
    "    \"languages\": {\n",
    "        \"latent_dim\": trainer_config.languages_dim,\n",
    "        \"output_dim\": len(dataset.langs_map) * dataset.num_lang_levels,\n",
    "        \"indexes\": (\n",
    "            trainer_config.skills_dim + trainer_config.education_dim,\n",
    "            trainer_config.skills_dim\n",
    "            + trainer_config.education_dim\n",
    "            + trainer_config.languages_dim,\n",
    "        ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(\"checkpoints\", EXPERIMENT, CHECKPOINT))\n",
    "\n",
    "candidate_vae = CandidateVAE(\n",
    "    general_config, encoder_config, decoder_config, dataset.vocab, dataset.embedder\n",
    ").to(DEVICE)\n",
    "\n",
    "candidate_vae.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "candidate_vae.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "candidate_vae.embedding.load_state_dict(checkpoint[\"embedding\"]) if checkpoint[\n",
    "    \"embedding\"\n",
    "] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CandidateVAE(\n",
       "  (encoder): CandidateEncoder(\n",
       "    (lstm): LSTM(100, 64, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fc_var): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): CandidateDecoder(\n",
       "    (lstm): LSTM(100, 64)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=100, bias=True)\n",
       "    )\n",
       "    (attn): Attn()\n",
       "    (attn_mu): Identity()\n",
       "    (attn_var): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (concat): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (out): Linear(in_features=64, out_features=28371, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_rows(dataset: SellersDataset) -> dict:\n",
    "    rows = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        latents = {}\n",
    "        # Both seeds have to me set up!!!\n",
    "        rng = np.random.default_rng(42)\n",
    "        random.seed(42)\n",
    "        row = dataset.__getitem__(idx)\n",
    "        targets = {}\n",
    "\n",
    "        (\n",
    "            input_tensor,\n",
    "            _,\n",
    "            targets[\"skills\"],\n",
    "            targets[\"education\"],\n",
    "            targets[\"languages\"],\n",
    "        ) = row\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_lengths = torch.tensor(len(input_tensor)).unsqueeze(dim=0)\n",
    "            mu, var, outputs, (hn, cn) = candidate_vae.encoder(\n",
    "                input_tensor.unsqueeze(dim=1).to(DEVICE), input_lengths.to(\"cpu\")\n",
    "            )\n",
    "            z = candidate_vae.decoder.reparameterize(mu, var)\n",
    "\n",
    "        for key in disentangled_targets:\n",
    "            index_start, index_end = disentangled_targets[key][\"indexes\"]\n",
    "\n",
    "            # Use mu or z?\n",
    "            # latents[key] = [mu[:, index_start:index_end], row[f\"{key}_vec\"]]\n",
    "            latents[key] = [\n",
    "                z[:, index_start:index_end],\n",
    "                torch.cat(\n",
    "                    (z[:, :index_start], z[:, index_end:]),\n",
    "                    dim=1,\n",
    "                ),\n",
    "                targets[key],\n",
    "            ]\n",
    "\n",
    "        rows.append(latents)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def prepare_test_data(dataset: SellersDataset):\n",
    "    # We have to set both seeds!!!\n",
    "    rng = np.random.default_rng(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    texts = dataset.test_dataset.progress_apply(\n",
    "        lambda x: dataset._create_textual_decription(x, rng), axis=1\n",
    "    )\n",
    "    embedded = [dataset.embedder(text)[0].cpu() for text in tqdm(texts)]\n",
    "\n",
    "    # if general_config.embedder_name != EmbedderType.LANG:\n",
    "    embedded = [text.unsqueeze(dim=1) for text in tqdm(embedded)]\n",
    "\n",
    "    input_lengths = [torch.tensor(len(row)).unsqueeze(dim=0) for row in embedded]\n",
    "\n",
    "    dataset.test_dataset[\"embedded\"] = embedded\n",
    "    dataset.test_dataset[\"input_lengths\"] = input_lengths\n",
    "\n",
    "\n",
    "def prepare_test_row(row: pd.Series) -> dict:\n",
    "    latents = {}\n",
    "    with torch.no_grad():\n",
    "        mu, var, outputs, (hn, cn) = candidate_vae.encoder(\n",
    "            row[\"embedded\"].to(DEVICE), row[\"input_lengths\"].to(\"cpu\")\n",
    "        )\n",
    "        z = candidate_vae.decoder.reparameterize(mu, var)\n",
    "\n",
    "    for key in disentangled_targets:\n",
    "        index_start, index_end = disentangled_targets[key][\"indexes\"]\n",
    "\n",
    "        # Use mu or z?\n",
    "        # latents[key] = [mu[:, index_start:index_end], row[f\"{key}_vec\"]]\n",
    "        latents[key] = [\n",
    "            z[:, index_start:index_end],\n",
    "            torch.cat(\n",
    "                (z[:, :index_start], z[:, index_end:]),\n",
    "                dim=1,\n",
    "            ),\n",
    "            row[f\"{key}_vec\"],\n",
    "        ]\n",
    "\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40014/40014 [01:38<00:00, 405.66it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4626.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15421.88it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 919803.51it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 699.17it/s]\n"
     ]
    }
   ],
   "source": [
    "if CREATE_DATASET:\n",
    "    train_latents = prepare_train_rows(dataset)\n",
    "    prepare_test_data(dataset)\n",
    "    test_latents = dataset.test_dataset.progress_apply(prepare_test_row, axis=1)\n",
    "    os.makedirs(\"tests/data/capacity\", exist_ok=True)\n",
    "    with open(os.path.join(\"tests/data/capacity/train_latents.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(train_latents, f)\n",
    "\n",
    "    with open(os.path.join(\"tests/data/capacity/test_latents.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(test_latents, f)\n",
    "else:\n",
    "    with open(os.path.join(\"tests/data/capacity/train_latents.pickle\"), \"rb\") as f:\n",
    "        train_latents = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(\"tests/data/capacity/test_latents.pickle\"), \"rb\") as f:\n",
    "        test_latents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, latents: dict, key: str):\n",
    "        self.data = [row[key] for row in latents]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "adversarial_datasets_train = {\n",
    "    target: AdversarialDataset(train_latents, target) for target in disentangled_targets\n",
    "}\n",
    "\n",
    "\n",
    "adversarial_datasets_test = {\n",
    "    target: AdversarialDataset(test_latents, target) for target in disentangled_targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {\n",
    "    target: DataLoader(\n",
    "        adversarial_datasets_train[target],\n",
    "        batch_size=4096,\n",
    "    )\n",
    "    for target in disentangled_targets\n",
    "}\n",
    "\n",
    "dataloaders_test = {\n",
    "    target: DataLoader(\n",
    "        adversarial_datasets_test[target],\n",
    "        batch_size=1024,\n",
    "    )\n",
    "    for target in disentangled_targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1, 48])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloaders_train[\"skills\"]))[1].shape\n",
    "# next(iter(dataloaders_test[\"languages\"]))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model: nn.Module,\n",
    "    loss_fn: torch.nn.CrossEntropyLoss,\n",
    "    dataloader: DataLoader,\n",
    "    _type: str = \"mult\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    loss = 0\n",
    "    # _all = 0\n",
    "    iters = 0\n",
    "    for X_mult_batch, X_adv_batch, y_batch in dataloader:\n",
    "\n",
    "        y_pred = model(\n",
    "            X_mult_batch.squeeze(dim=1).cuda()\n",
    "            if _type == \"mult\"\n",
    "            else X_adv_batch.squeeze(dim=1).cuda()\n",
    "        )\n",
    "        iters += 1\n",
    "        loss += loss_fn(y_pred, y_batch.cuda())\n",
    "    return loss / iters\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    writer: SummaryWriter,\n",
    "    epochs: int = 20,\n",
    "    print_metrics: bool = True,\n",
    "    patience: int = 5,\n",
    "    run_prefix: str = \"early_stopping\",\n",
    "    _type: str = \"mult\",\n",
    ") -> dict[str, list]:\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "\n",
    "        for X_mult_batch, X_adv_batch, y_batch in train_dl:\n",
    "            X_batch = X_mult_batch if _type == \"mult\" else X_adv_batch\n",
    "            X_batch, y_batch = (\n",
    "                X_batch.squeeze(dim=1).cuda(),\n",
    "                y_batch.cuda(),\n",
    "            )\n",
    "            y_pred = model(\n",
    "                X_batch\n",
    "            )  # Uzyskanie pseudoprawdopodobieństw dla próbek z minibatcha\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)  # Policzenie funkcji straty\n",
    "            loss.backward()  # Wsteczna propagacja z wyniku funkcji straty - policzenie gradientów i zapisanie ich w tensorach (parametrach)\n",
    "            optimizer.step()  # Aktualizacja parametrów modelu przez optymalizator na podstawie gradientów zapisanych w tensorach (parametrach) oraz lr\n",
    "            optimizer.zero_grad()  # Wyzerowanie gradientów w modelu, alternatywnie można wywołać model.zero_grad()\n",
    "\n",
    "        model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "        with torch.no_grad():  # Wstrzymujemy przeliczanie i śledzenie gradientów dla tensorów - w procesie ewaluacji modelu nie chcemy zmian w gradientach\n",
    "            train_loss = validate(model, loss_fn, train_dl, _type)\n",
    "            val_loss = validate(model, loss_fn, val_dl, _type)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                current_patience = 0\n",
    "                os.makedirs(\"tests/checkpoints/capacity\", exist_ok=True)\n",
    "                torch.save(\n",
    "                    obj={\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    f=\"tests/checkpoints/capacity/best\" + \"_\" + run_prefix,\n",
    "                )\n",
    "            else:\n",
    "                current_patience += 1\n",
    "\n",
    "        losses[\"train\"].append(train_loss)\n",
    "        losses[\"val\"].append(val_loss)\n",
    "\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix}/loss\",\n",
    "            tag_scalar_dict={\"train\": train_loss, \"dev\": val_loss},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "\n",
    "        if print_metrics:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: \"\n",
    "                f\"train loss = {train_loss:.3f}, \"\n",
    "                f\"validation loss = {val_loss:.3f}\"\n",
    "            )\n",
    "\n",
    "        if current_patience >= patience:\n",
    "            break\n",
    "\n",
    "    model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "    # return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define networks to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossentropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "multitask_classifiers = nn.ModuleDict(\n",
    "    {\n",
    "        target: nn.Linear(\n",
    "            disentangled_targets[target][\"latent_dim\"],\n",
    "            disentangled_targets[target][\"output_dim\"],\n",
    "        ).to(DEVICE)\n",
    "        for target in disentangled_targets\n",
    "    }\n",
    ")\n",
    "# Retreiving target using all except target. Classifiers should fail :)\n",
    "adversarial_classifiers = nn.ModuleDict(\n",
    "    {\n",
    "        target: nn.Linear(\n",
    "            general_config.latent_dim - disentangled_targets[target][\"latent_dim\"],\n",
    "            disentangled_targets[target][\"output_dim\"],\n",
    "        ).to(DEVICE)\n",
    "        for target in disentangled_targets\n",
    "    }\n",
    ")\n",
    "\n",
    "multitask_optimizers = {\n",
    "    target: torch.optim.Adam(\n",
    "        multitask_classifiers[target].parameters(),\n",
    "        lr=0.05,\n",
    "    )\n",
    "    for target in disentangled_targets\n",
    "}\n",
    "\n",
    "adversarial_optimizers = {\n",
    "    target: torch.optim.Adam(\n",
    "        adversarial_classifiers[target].parameters(),\n",
    "        lr=0.05,\n",
    "    )\n",
    "    for target in disentangled_targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:55<00:28,  1.45s/it]\n",
      " 50%|█████     | 50/100 [01:18<01:18,  1.56s/it]\n",
      " 23%|██▎       | 23/100 [00:39<02:13,  1.73s/it]\n",
      " 23%|██▎       | 23/100 [00:38<02:10,  1.69s/it]\n",
      " 82%|████████▏ | 82/100 [00:16<00:03,  4.88it/s]\n",
      " 46%|████▌     | 46/100 [00:09<00:11,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "for target in disentangled_targets:\n",
    "    _type = \"mult\"\n",
    "    fit(\n",
    "        model=multitask_classifiers[target],\n",
    "        optimizer=multitask_optimizers[target],\n",
    "        loss_fn=crossentropy_loss,\n",
    "        train_dl=dataloaders_train[target],\n",
    "        val_dl=dataloaders_test[target],\n",
    "        writer=writer_tensorboard,\n",
    "        epochs=EPOCHS,\n",
    "        print_metrics=False,\n",
    "        patience=10,\n",
    "        run_prefix=f\"capacity_{_type}_{target}_{EXPERIMENT}_{CHECKPOINT.replace('.tar', '')}\",\n",
    "        _type=_type,\n",
    "    )\n",
    "    _type = \"adv\"\n",
    "    fit(\n",
    "        model=adversarial_classifiers[target],\n",
    "        optimizer=adversarial_optimizers[target],\n",
    "        loss_fn=crossentropy_loss,\n",
    "        train_dl=dataloaders_train[target],\n",
    "        val_dl=dataloaders_test[target],\n",
    "        writer=writer_tensorboard,\n",
    "        epochs=EPOCHS,\n",
    "        print_metrics=False,\n",
    "        patience=10,\n",
    "        run_prefix=f\"capacity_{_type}_{target}_{EXPERIMENT}_{CHECKPOINT.replace('.tar', '')}\",\n",
    "        _type=_type,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a62dc5c4e15874d230a4f396bb129d37f109e576af212db1e92385126337577"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
