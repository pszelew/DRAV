{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt to create a new representation method for a candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1, 40])\n",
      "Hidden shape: torch.Size([2, 1, 20])\n",
      "Cell shape: torch.Size([2, 1, 20])\n",
      "Cat shape torch.Size([2, 1, 40])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 1, 25])\n",
      "torch.Size([1, 1, 25])\n",
      "tensor([[0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, bidirectional = True):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.input_size = input_size\n",
    "    self.bidirectional = bidirectional\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = bidirectional)\n",
    "  \n",
    "  def forward(self, inputs, hidden):\n",
    "    \n",
    "    output, hidden = self.lstm(inputs.view(1, 1, self.input_size), hidden)\n",
    "    return output, hidden\n",
    "    \n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\n",
    "      torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, hidden_size, output_size, vocab_size):\n",
    "    super(AttentionDecoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    \n",
    "    self.attn = nn.Linear(hidden_size + output_size, 1)\n",
    "    self.lstm = nn.LSTM(hidden_size + vocab_size, output_size) #if we are using embedding hidden_size should be added with embedding of vocab size\n",
    "    self.final = nn.Linear(output_size, vocab_size)\n",
    "  \n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1, 1, self.output_size),\n",
    "      torch.zeros(1, 1, self.output_size))\n",
    "  \n",
    "  def forward(self, decoder_hidden, encoder_outputs, input):\n",
    "    \n",
    "    weights = []\n",
    "    for i in range(len(encoder_outputs)):\n",
    "      print(decoder_hidden[0][0].shape)\n",
    "      print(encoder_outputs[0].shape)\n",
    "      weights.append(self.attn(torch.cat((decoder_hidden[0][0], \n",
    "                                          encoder_outputs[i]), dim = 1)))\n",
    "    normalized_weights = F.softmax(torch.cat(weights, 1), 1)\n",
    "    \n",
    "    attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n",
    "                             encoder_outputs.view(1, -1, self.hidden_size))\n",
    "    \n",
    "    input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\n",
    "    \n",
    "    output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\n",
    "    \n",
    "    output = self.final(output[0])\n",
    "    \n",
    "    return output, hidden, normalized_weights\n",
    "  \n",
    "\n",
    "bidirectional = True\n",
    "c = Encoder(10, 20, bidirectional)\n",
    "a, b = c.forward(torch.randn(10), c.init_hidden())\n",
    "print(f\"Output shape: {a.shape}\")\n",
    "print(f\"Hidden shape: {b[0].shape}\")\n",
    "print(f\"Cell shape: {b[1].shape}\")\n",
    "print(f\"Cat shape {torch.cat((a,a)).shape}\")\n",
    "\n",
    "x = AttentionDecoder(20 * (1 + bidirectional), 25, 30)\n",
    "y, z, w = x.forward(x.init_hidden(), torch.cat((a,a)), torch.zeros(1,1,30)) #Assuming <SOS> to be all zeros\n",
    "print(y.shape)\n",
    "print(z[0].shape)\n",
    "print(z[1].shape)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1335, -0.0468, -0.1223,  0.0402,  0.1056, -0.2290, -0.2726,\n",
       "           0.0667,  0.0794,  0.0335,  0.0589, -0.0310,  0.1119,  0.0896,\n",
       "           0.1737, -0.0317, -0.0732,  0.2047,  0.0522, -0.1213,  0.1520,\n",
       "           0.1665,  0.1283,  0.0568,  0.2498, -0.2609, -0.1174, -0.1244,\n",
       "          -0.0251,  0.0219, -0.0843, -0.0520, -0.0463, -0.0383,  0.1047,\n",
       "           0.1322,  0.0649,  0.1311,  0.1106,  0.0522]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1335, -0.0468, -0.1223,  0.0402,  0.1056, -0.2290, -0.2726,\n",
       "           0.0667,  0.0794,  0.0335,  0.0589, -0.0310,  0.1119,  0.0896,\n",
       "           0.1737, -0.0317, -0.0732,  0.2047,  0.0522, -0.1213]],\n",
       "\n",
       "        [[ 0.1520,  0.1665,  0.1283,  0.0568,  0.2498, -0.2609, -0.1174,\n",
       "          -0.1244, -0.0251,  0.0219, -0.0843, -0.0520, -0.0463, -0.0383,\n",
       "           0.1047,  0.1322,  0.0649,  0.1311,  0.1106,  0.0522]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "\n",
    "from model.encoder import CandidateEncoder, CandidateEncoderConfig\n",
    "from model.decoder import CandidateDecoder, CandidateDecoderConfig\n",
    "from config.general_config import GeneralConfig\n",
    "from model.embedder import EmbedderType\n",
    "from dataset.utils import pad_collate\n",
    "from dataset.dataset import SellersDataset\n",
    "from dataset.language import Lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(\"config/config.yaml\", \"r\") as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)[\"vae\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "GENERAL_CONFIG = GeneralConfig(**config[\"general\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,025] {dataset.py:133} INFO - Preparing dataset\n",
      "[2022-05-18 21:13:48,025] {dataset.py:160} INFO - Detecting languages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,317] {dataset.py:165} INFO - Detected languages:\n",
      "[2022-05-18 21:13:48,318] {dataset.py:166} INFO - lang\n",
      "en    4\n",
      "Name: lang, dtype: int64\n",
      "[2022-05-18 21:13:48,319] {dataset.py:167} INFO - Removing rows not written in english\n",
      "[2022-05-18 21:13:48,320] {dataset.py:171} INFO - Removed 0 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 4847.51it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5113.45it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1175.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 9956.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,331] {dataset.py:347} INFO - Adding language for languages_str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 13617.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,333] {dataset.py:347} INFO - Adding language for education_str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 22104.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,334] {dataset.py:347} INFO - Adding language for skills_str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 16024.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-18 21:13:48,336] {dataset.py:347} INFO - Adding language for description_str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 9980.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = SellersDataset(\n",
    "    embedder_name=config[\"general\"][\"embedder_name\"], data_path=config[\"general\"][\"data_path\"], device=DEVICE\n",
    ")\n",
    "dataset.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=GENERAL_CONFIG.batch_size, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_encoder = CandidateEncoderConfig(\n",
    "    num_words=dataset.lang.n_words,\n",
    "    embedding_size=dataset.embedder.size,\n",
    "    device=DEVICE,\n",
    "    **config[\"encoder\"],\n",
    "    **config[\"general\"]\n",
    ")\n",
    "\n",
    "encoder = CandidateEncoder(config_encoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/patryk/TOSHIBA EXT/Studia/SztucznaInteligencja/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py:329: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[\"skills_str\"] = (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 16]), torch.Size([3, 16]), torch.Size([3, 134, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, var, outputs = encoder(next(iter(loader)).cuda())\n",
    "mu.shape, var.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Will a single z be enough ti compute the expectation\n",
    "    for the loss??\n",
    "    :param mu: (Tensor) Mean of the latent Gaussian\n",
    "    :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vector = reparameterize(mu, var)\n",
    "latent_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_decoder = CandidateDecoderConfig(\n",
    "    num_words=dataset.lang.n_words,\n",
    "    embedding_size=dataset.embedder.size,\n",
    "    device=DEVICE,\n",
    "    **config[\"decoder\"],\n",
    "    **config[\"general\"]\n",
    ")\n",
    "\n",
    "decoder = CandidateDecoder(config_decoder).to(DEVICE)\n",
    "\n",
    "init_hidden = decoder.init_hidden_cell(GENERAL_CONFIG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_strip_sequence(batch):\n",
    "    template = torch.zeros(GENERAL_CONFIG.max_seq_len, batch.shape[-1], device=DEVICE)\n",
    "    return pad_sequence([template, *batch], batch_first=True)[1:,:GENERAL_CONFIG.max_seq_len,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_strip_sequence(outputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden, attn_weights = decoder(latent_vector, init_hidden, pad_strip_sequence(outputs), True)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a62dc5c4e15874d230a4f396bb129d37f109e576af212db1e92385126337577"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
