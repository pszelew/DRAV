# Configuration for CandidateVAE

vae:
  general:
    # ------ Train specific -----
    # Save every n batches  
    save_every: 1000
    # Log loss every n batches
    log_every: 100
    # Print to console
    print_console: False
    # Batch size
    batch_size: 32
    # Train epochs. n_iters is train_epochs * dataset.size / batch_size
    train_epochs: 6
    # Encoder learning rate
    encoder_lr: 0.00001
    # Decoder learning rate
    decoder_lr: 0.00005
    # Directory with dataset files (or to be created there)
    datset_path: data/dataset_2/
    # Test index
    test_index: test_2.index
    # Path to raw dataset file
    raw_data_path: data/extracted_sellers_old.json
    # Path for training checkpoints
    checkpoints_dir: checkpoints/
    # Bag of words - remove stopwords
    bow_remove_stopwords: True
    # Bag of words - remove sentiment words
    # https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
    bow_remove_sentiment: True
    # Trim words rarer than this value from dataset.vocab
    trim_tr: 2

    # ------ Shared config -----
    # One of lang/fasttext/roberta
    embedder_name: fasttext
    # Longer texts will be trimmed and shorter padded to this value
    # Currently not used!!!!
    max_seq_len: 256
    # Hidden dims of LSTMs used in encoder and decoder
    lstm_hidden_dim: 64
    # Shape of a latent vector in VAE
    latent_dim: 64


  encoder:
    # If True encoder will be bidirectional.
    bidirectional: True
    # Number of layers in lstm network
    num_layers_lstm: 1
    # Hidden linear layers in encoder. Applied between LSTM and (fc_mu, fc_var)
    hidden_dims: [64]
    # Dropout applies to outputs of hidden layers
    dropout: 0.1
  decoder:
    # Number of layers in lstm network of decoder
    num_layers_lstm: 1
    # Number of layers in lstm network of decoder
    hidden_dims: [64]
    # Dropout applied to relu layers
    dropout: 0.1
    # If true variational attention will be used instead of a deterministic one
    # https://arxiv.org/pdf/1712.08207.pdf
    vattn: True
    # dot / general / concat
    # https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#run-model
    attn_method: dot
    # Bypass mechanism is described here https://arxiv.org/pdf/1712.08207.pdf
    bypassing: False
  trainer:
    # 'standard' is a basic beta vae. Uses beta param
    # 'capacity' is is a beta with with controlled capacity increase proposed in
    # uses gamma param. Currently not scheduled
    loss_type: standard    
    # batch_size/num_of_examples?
    # https://github.com/AntixK/PyTorch-VAE/issues/11
    # If 1 it is disabled
    kld_weight: 1
    # Used only if loss_type is standard
    beta: 1
    # Used only if loss_type is capacity
    gamma: 1000.0
    # https://arxiv.org/pdf/1804.03599.pdf
    # Used only if loss_type is capacity
    max_capacity: 25
    # Used only if loss_type is capacity
    capacity_max_iter: 100000

    # This is gamma a for variational attention
    # https://arxiv.org/pdf/1712.08207.pdf
    gamma_a: 1
    
    # From paper https://arxiv.org/pdf/1903.10145.pdf
    use_beta_cycle: True
    # Number of cycles
    # From paper https://arxiv.org/pdf/1903.10145.pdf
    n_cycle: 10
    # Ratio of increasing Beta in each cycle. 
    # From paper https://arxiv.org/pdf/1903.10145.pdf
    ratio_increase: 0.25
    # Ratio of zero Beta in each cycle. 
    # From paper https://arxiv.org/pdf/1903.10145.pdf
    ratio_zero: 0.25

    # Free bit vae loss
    # from paper https://arxiv.org/pdf/1606.04934.pdf
    free_bit_kl: False
    # Lambda value from paper https://arxiv.org/pdf/1606.04934.pdf
    lambda_target_kl: 0.5

    # Clip value for gradients
    # https://www.deeplearningbook.org/
    use_clip: True
    clip: 50.0

    # https://arxiv.org/abs/1808.04339v2
    # If true then use multitask and adversarial losses from 
    use_disentangled_loss: True
    # lambda coeficient weighting multitask loss for skills and
    # lambda coeficient weighting adversarial loss for skills
    lambda_mul_skills: 0.02
    lambda_adv_skills: 1.0
    adv_skills_lr: 0.00001
    # lambda coeficient weighting multitask loss for education and
    # lambda coeficient weighting adversarial loss for education
    lambda_mul_education: 0.02
    lambda_adv_education: 1.0
    adv_education_lr: 0.00001
    # lambda coeficient weighting multitask loss for languages and
    # lambda coeficient weighting adversarial loss for education
    lambda_mul_languages: 0.02
    lambda_adv_languages: 1.0
    adv_languages_lr: 0.00001
    
    
    # Skills dim
    # [:skills_dim] dimensions should code skills
    # if trainer.use_disentangled_loss == True
    skills_dim: 16
    # Education dim
    # [skills_dim:skills_dim+education_dim] dimensions should code education
    #  if trainer.use_disentangled_loss == True
    education_dim: 16
    # Languages dim
    # [skills_dim + education_dim:skills_dim + education_dim + languages_dim]
    # dimensions should code education if trainer.use_disentangled_loss == True
    languages_dim: 16
    # The rest is for content
    # [skills_dim + education_dim + languages_dim:latent_dim]
    # content_dim = latent_dim - (skills_dim + education_dim + languages_dim)


