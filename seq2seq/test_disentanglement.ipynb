{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test disentanglement\n",
    "\n",
    "Test if we can infer about certain type of competences using only the desired part of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from model.encoder import CandidateEncoderConfig\n",
    "from model.decoder import CandidateDecoderConfig\n",
    "from model.candidate_vae import CandidateVAE\n",
    "from trainer.trainer import TrainerConfig\n",
    "from config.general_config import GeneralConfig\n",
    "from dataset.dataset import SellersDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = \"candidate_vae_04_06_22_01_46_39\"\n",
    "CHECKPOINT = \"7506_checkpoint.tar\"\n",
    "# If false, we can used cached content e.g. if we are testing the code\n",
    "CREATE_DATASET = True\n",
    "LR = 0.001\n",
    "PATIENCE = 10\n",
    "VALIDATE_EVERY = 50\n",
    "LABEL_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(os.path.join(\"checkpoints\", EXPERIMENT, \"config.yaml\"), \"r\") as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)[\"vae\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "general_config = GeneralConfig(**config[\"general\"])\n",
    "\n",
    "\n",
    "encoder_config = CandidateEncoderConfig(**{**config[\"encoder\"], **config[\"general\"]})\n",
    "\n",
    "decoder_config = CandidateDecoderConfig(**{**config[\"decoder\"], **config[\"general\"]})\n",
    "\n",
    "trainer_config = TrainerConfig(**{**config[\"trainer\"], **config[\"general\"]})\n",
    "\n",
    "log_dir = os.path.join(general_config.checkpoints_dir, \"runs\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir $log_dir --port=6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset data/dataset_3/...\n",
      "[2022-06-04 17:33:40,260] {dataset.py:260} INFO - Loading dataset data/dataset_3/...\n",
      "Loaded dataset data/dataset_3/!\n",
      "[2022-06-04 17:33:40,777] {dataset.py:288} INFO - Loaded dataset data/dataset_3/!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataset = SellersDataset(\n",
    "    dataset_path=general_config.datset_path,\n",
    "    test_index=general_config.test_index,\n",
    "    embedder_name=general_config.embedder_name,\n",
    "    raw_data_path=general_config.raw_data_path,\n",
    "    device=DEVICE,\n",
    "    bow_remove_stopwords=general_config.bow_remove_stopwords,\n",
    "    bow_remove_sentiment=general_config.bow_remove_sentiment,\n",
    "    nn_embedding_size=encoder_config.lstm_hidden_dim,\n",
    "    trim_tr=general_config.trim_tr,\n",
    ")\n",
    "# dataset.prepare_dataset()\n",
    "dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(\"checkpoints\", EXPERIMENT, CHECKPOINT))\n",
    "\n",
    "candidate_vae = CandidateVAE(\n",
    "    general_config, encoder_config, decoder_config, dataset.vocab, dataset.embedder\n",
    ").to(DEVICE)\n",
    "\n",
    "candidate_vae.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "candidate_vae.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "candidate_vae.embedding.load_state_dict(checkpoint[\"embedding\"]) if checkpoint[\n",
    "    \"embedding\"\n",
    "] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CandidateVAE(\n",
       "  (encoder): CandidateEncoder(\n",
       "    (lstm): LSTM(100, 256, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_var): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): CandidateDecoder(\n",
       "    (lstm): LSTM(100, 256)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=100, bias=True)\n",
       "    )\n",
       "    (attn): Attn()\n",
       "    (attn_mu): Identity()\n",
       "    (attn_var): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (concat): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=20522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_batch(dataset: SellersDataset, batch_size: int) -> dict:\n",
    "    factor_labels = dataset.string_keys\n",
    "    source_idx = random.randint(0, len(dataset) - 1)\n",
    "    fixed_factor_idx = random.randint(0, len(factor_labels) - 1)\n",
    "    fixed_factor_label = factor_labels[fixed_factor_idx]\n",
    "    fixed_factor = dataset._load_row(source_idx)\n",
    "\n",
    "    inputs = []\n",
    "    for _ in range(batch_size * 2):\n",
    "        x = deepcopy(fixed_factor)\n",
    "        z_s = []\n",
    "\n",
    "        for column in [fact for fact in factor_labels if fact != fixed_factor_label]:\n",
    "            source = random.randint(0, len(dataset) - 1)\n",
    "            x[column] = dataset.get_column_by_idx(source, column)\n",
    "\n",
    "        x_str = dataset._create_textual_decription(x)\n",
    "        x_emb = dataset.embedder(x_str)[0].cpu()\n",
    "        inputs.append((x_emb, torch.tensor(len(x_emb))))\n",
    "\n",
    "    inputs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    input_tensors, input_lens = zip(*inputs)\n",
    "    input_pad = pad_sequence(input_tensors, padding_value=dataset.vocab.pad_token)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, var, outputs, (hn, cn) = candidate_vae.encoder(\n",
    "            input_pad.to(DEVICE), input_lens\n",
    "        )\n",
    "        z_s = candidate_vae.decoder.reparameterize(mu, var)\n",
    "        latent_dim = z_s.shape[1]\n",
    "        z_s = z_s.view(batch_size, -1)\n",
    "        z_s = torch.abs(z_s[:, :latent_dim] - z_s[:, latent_dim:])\n",
    "\n",
    "    return z_s.mean(dim=0), fixed_factor_idx\n",
    "\n",
    "\n",
    "def prepare_test_batch(dataset: SellersDataset, batch_size: int):\n",
    "    # We have to set both seeds!!!\n",
    "    # rng = np.random.default_rng(42)\n",
    "    # random.seed(42)\n",
    "\n",
    "    factor_labels = dataset.string_keys\n",
    "    source_idx = random.randint(0, dataset.test_size - 1)\n",
    "    fixed_factor_idx = random.randint(0, len(factor_labels) - 1)\n",
    "    fixed_factor_label = factor_labels[fixed_factor_idx]\n",
    "    fixed_factor = dataset.test_dataset.iloc[source_idx]\n",
    "\n",
    "    inputs = []\n",
    "\n",
    "    for _ in range(batch_size * 2):\n",
    "        x = deepcopy(fixed_factor)\n",
    "        z_s = []\n",
    "\n",
    "        for column in [fact for fact in factor_labels if fact != fixed_factor_label]:\n",
    "            source = random.randint(0, dataset.test_size - 1)\n",
    "            x[column] = dataset.get_column_by_idx(source, column)\n",
    "\n",
    "        x_str = dataset._create_textual_decription(x)\n",
    "        x_emb = dataset.embedder(x_str)[0].cpu()\n",
    "        inputs.append((x_emb, torch.tensor(len(x_emb))))\n",
    "\n",
    "    inputs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    input_tensors, input_lens = zip(*inputs)\n",
    "    input_pad = pad_sequence(input_tensors, padding_value=dataset.vocab.pad_token)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, var, outputs, (hn, cn) = candidate_vae.encoder(\n",
    "            input_pad.to(DEVICE), input_lens\n",
    "        )\n",
    "        z_s = candidate_vae.decoder.reparameterize(mu, var)\n",
    "        latent_dim = z_s.shape[1]\n",
    "        z_s = z_s.view(batch_size, -1)\n",
    "        z_s = torch.abs(z_s[:, :latent_dim] - z_s[:, latent_dim:])\n",
    "\n",
    "    return z_s.mean(dim=0), fixed_factor_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sellers_dataset: SellersDataset, _type: str = \"train\"):\n",
    "        self._type = _type\n",
    "        self.sellers_dataset = sellers_dataset\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return (\n",
    "            prepare_train_batch(self.sellers_dataset, LABEL_BATCH_SIZE)\n",
    "            if self._type == \"train\"\n",
    "            else prepare_test_batch(self.sellers_dataset, LABEL_BATCH_SIZE)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(1e11)\n",
    "\n",
    "\n",
    "train_dataset = AdversarialDataset(dataset, \"train\")\n",
    "test_dataset = AdversarialDataset(dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return recall_score(\n",
    "        y_true.cpu(), preds.cpu(), labels=list(range(0, 4)), average=\"micro\"\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_fscore(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return f1_score(\n",
    "        y_true.cpu(), preds.cpu(), average=\"micro\", labels=list(range(0, 4))\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_precission(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return precision_score(\n",
    "        y_true.cpu(), preds.cpu(), average=\"micro\", labels=list(range(0, 4))\n",
    "    )\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "def calc_recall(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return recall_score(\n",
    "        y_true.cpu(), preds.cpu(), average=\"micro\", labels=list(range(0, 4))\n",
    "    )\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    loss_fn: torch.nn.CrossEntropyLoss,\n",
    "    dataloader: DataLoader,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    f_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    # _all = 0\n",
    "    iters = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        iters += 1\n",
    "\n",
    "        loss += loss_fn(y_pred, y_batch.cuda())\n",
    "        accuracy += calc_accuracy(y_pred, y_batch)\n",
    "        f_score += calc_fscore(y_pred, y_batch)\n",
    "        precision += calc_precission(y_pred, y_batch)\n",
    "        recall += calc_recall(y_pred, y_batch)\n",
    "        if iters == 10:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        loss / iters,\n",
    "        accuracy / iters,\n",
    "        f_score / iters,\n",
    "        precision / iters,\n",
    "        recall / iters,\n",
    "    )\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    writer: SummaryWriter,\n",
    "    validate_every: int = 100,\n",
    "    print_metrics: bool = True,\n",
    "    patience: int = 5,\n",
    "    run_prefix: str = \"early_stopping\",\n",
    ") -> dict[str, list]:\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "    accs = {\"train\": [], \"val\": []}\n",
    "    f1s = {\"train\": [], \"val\": []}\n",
    "    precisions = {\"train\": [], \"val\": []}\n",
    "    recalls = {\"train\": [], \"val\": []}\n",
    "\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    iter = 0\n",
    "    for X_batch, y_batch in tqdm(train_dl):\n",
    "        iter += 1\n",
    "        model.train()\n",
    "        X_batch, y_batch = (\n",
    "            X_batch.cuda(),\n",
    "            y_batch.cuda(),\n",
    "        )\n",
    "        y_pred = model(\n",
    "            X_batch\n",
    "        )  # Uzyskanie pseudoprawdopodobieństw dla próbek z minibatcha\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)  # Policzenie funkcji straty\n",
    "        loss.backward()  # Wsteczna propagacja z wyniku funkcji straty - policzenie gradientów i zapisanie ich w tensorach (parametrach)\n",
    "        optimizer.step()  # Aktualizacja parametrów modelu przez optymalizator na podstawie gradientów zapisanych w tensorach (parametrach) oraz lr\n",
    "        optimizer.zero_grad()  # Wyzerowanie gradientów w modelu, alternatywnie można wywołać model.zero_grad()\n",
    "\n",
    "        model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "        if iter % validate_every == 0:\n",
    "            with torch.no_grad():  # Wstrzymujemy przeliczanie i śledzenie gradientów dla tensorów - w procesie ewaluacji modelu nie chcemy zmian w gradientach\n",
    "                (\n",
    "                    train_loss,\n",
    "                    train_acc,\n",
    "                    train_f1,\n",
    "                    train_precision,\n",
    "                    train_recall,\n",
    "                ) = validate(model, loss_fn, train_dl)\n",
    "                # val_loss, val_acc, val_f1, val_precision, val_recall = validate(model, loss_fn, val_dl)\n",
    "\n",
    "                if train_loss < min_val_loss:\n",
    "                    min_val_loss = train_loss\n",
    "                    current_patience = 0\n",
    "                    os.makedirs(\"tests/checkpoints/disentanglement\", exist_ok=True)\n",
    "                    torch.save(\n",
    "                        obj={\n",
    "                            \"iter\": iter,\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        },\n",
    "                        f=\"tests/checkpoints/disentanglement/best\" + \"_\" + run_prefix,\n",
    "                    )\n",
    "                else:\n",
    "                    current_patience += 1\n",
    "\n",
    "            losses[\"train\"].append(train_loss)\n",
    "            accs[\"train\"].append(train_acc)\n",
    "            f1s[\"train\"].append(train_f1)\n",
    "            precisions[\"train\"].append(train_precision)\n",
    "            recalls[\"train\"].append(train_recall)\n",
    "\n",
    "            # losses[\"val\"].append(val_loss)\n",
    "            # accs[\"val\"].append(val_acc)\n",
    "            # f1s[\"val\"].append(val_f1)\n",
    "            # precisions[\"val\"].append(val_precision)\n",
    "            # recalls[\"val\"].append(val_recall)\n",
    "\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"{run_prefix}/loss\",\n",
    "                tag_scalar_dict={\"train\": train_loss},\n",
    "                global_step=iter,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"{run_prefix}/acc\",\n",
    "                tag_scalar_dict={\"train\": train_acc},\n",
    "                global_step=iter,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"{run_prefix}/f1\",\n",
    "                tag_scalar_dict={\"train\": train_f1},\n",
    "                global_step=iter,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"{run_prefix}/precision\",\n",
    "                tag_scalar_dict={\"train\": train_precision},\n",
    "                global_step=iter,\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                main_tag=f\"{run_prefix}/recall\",\n",
    "                tag_scalar_dict={\"train\": train_recall},\n",
    "                global_step=iter,\n",
    "            )\n",
    "\n",
    "            if print_metrics:\n",
    "                print(\n",
    "                    f\"Iter {iter}: \"\n",
    "                    f\"train loss = {train_loss:.3f}, \"\n",
    "                    # f\"validation loss = {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "            if current_patience >= patience:\n",
    "                break\n",
    "\n",
    "    model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "    # return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define networks to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossentropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classifier = nn.Linear(general_config.latent_dim, 4).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 400/1562500000 [34:57<2276224:17:03,  5.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb Cell 21'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=0'>1</a>\u001b[0m fit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mclassifier,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=2'>3</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=3'>4</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mcrossentropy_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=4'>5</a>\u001b[0m     train_dl\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=5'>6</a>\u001b[0m     val_dl\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=6'>7</a>\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter_tensorboard,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=7'>8</a>\u001b[0m     validate_every\u001b[39m=\u001b[39;49mVALIDATE_EVERY,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=8'>9</a>\u001b[0m     print_metrics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=9'>10</a>\u001b[0m     patience\u001b[39m=\u001b[39;49mPATIENCE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=10'>11</a>\u001b[0m     run_prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdisentanglement_\u001b[39;49m\u001b[39m{\u001b[39;49;00mEXPERIMENT\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mCHECKPOINT\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m.tar\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000023?line=11'>12</a>\u001b[0m )\n",
      "\u001b[1;32m/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb Cell 18'\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, optimizer, loss_fn, train_dl, val_dl, writer, validate_every, print_metrics, patience, run_prefix)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000020?line=83'>84</a>\u001b[0m current_patience \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000020?line=84'>85</a>\u001b[0m \u001b[39miter\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000020?line=85'>86</a>\u001b[0m \u001b[39mfor\u001b[39;00m X_batch, y_batch \u001b[39min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000020?line=86'>87</a>\u001b[0m     \u001b[39miter\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000020?line=87'>88</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb Cell 15'\u001b[0m in \u001b[0;36mAdversarialDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=7'>8</a>\u001b[0m         prepare_train_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msellers_dataset, LABEL_BATCH_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=8'>9</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=9'>10</a>\u001b[0m         \u001b[39melse\u001b[39;00m prepare_test_batch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msellers_dataset, LABEL_BATCH_SIZE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000016?line=10'>11</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb Cell 14'\u001b[0m in \u001b[0;36mprepare_train_batch\u001b[0;34m(dataset, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000014?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m [fact \u001b[39mfor\u001b[39;00m fact \u001b[39min\u001b[39;00m factor_labels \u001b[39mif\u001b[39;00m fact \u001b[39m!=\u001b[39m fixed_factor_label]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000014?line=13'>14</a>\u001b[0m     source \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(dataset) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000014?line=14'>15</a>\u001b[0m     x[column] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mget_column_by_idx(source, column)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000014?line=16'>17</a>\u001b[0m x_str \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39m_create_textual_decription(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/test_disentanglement.ipynb#ch0000014?line=17'>18</a>\u001b[0m x_emb \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39membedder(x_str)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py:140\u001b[0m, in \u001b[0;36mSellersDataset.get_column_by_idx\u001b[0;34m(self, idx, label)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_column_by_idx\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m, label: \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=139'>140</a>\u001b[0m     data_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_row(idx)\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=140'>141</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data_row[label]\n",
      "File \u001b[0;32m~/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py:121\u001b[0m, in \u001b[0;36mSellersDataset._load_row\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=116'>117</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_row\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=117'>118</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=118'>119</a>\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdocument_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m:\u001b[39;00m\u001b[39m05\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=119'>120</a>\u001b[0m     ) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> <a href='file:///home/patryk/Studia/PracaMagisterska/master-thesis/seq2seq/dataset/dataset.py?line=120'>121</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39;49mload(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py:255\u001b[0m, in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=250'>251</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m d \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m d:\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=251'>252</a>\u001b[0m     \u001b[39m# Prevent Index.__new__ from conducting inference;\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=252'>253</a>\u001b[0m     \u001b[39m#  \"data\" key not in RangeIndex\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=253'>254</a>\u001b[0m     d[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m d[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=254'>255</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49md)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py:505\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=502'>503</a>\u001b[0m     arr \u001b[39m=\u001b[39m klass\u001b[39m.\u001b[39m_ensure_array(arr, dtype, copy)\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=503'>504</a>\u001b[0m     disallow_kwargs(kwargs)\n\u001b[0;32m--> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=504'>505</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m klass\u001b[39m.\u001b[39;49m_simple_new(arr, name)\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=506'>507</a>\u001b[0m \u001b[39melif\u001b[39;00m is_scalar(data):\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=507'>508</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_scalar_data_error(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py:668\u001b[0m, in \u001b[0;36mIndex._simple_new\u001b[0;34m(cls, values, name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=665'>666</a>\u001b[0m result\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=666'>667</a>\u001b[0m result\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=667'>668</a>\u001b[0m result\u001b[39m.\u001b[39;49m_reset_identity()\n\u001b[1;32m    <a href='file:///home/patryk/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=669'>670</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(\n",
    "    model=classifier,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=crossentropy_loss,\n",
    "    train_dl=train_dataloader,\n",
    "    val_dl=test_dataloader,\n",
    "    writer=writer_tensorboard,\n",
    "    validate_every=VALIDATE_EVERY,\n",
    "    print_metrics=False,\n",
    "    patience=PATIENCE,\n",
    "    run_prefix=f\"disentanglement_{EXPERIMENT}_{CHECKPOINT.replace('.tar', '')}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a62dc5c4e15874d230a4f396bb129d37f109e576af212db1e92385126337577"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
